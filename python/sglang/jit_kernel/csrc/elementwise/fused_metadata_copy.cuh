/*
 * Fused metadata copy kernel for NSA backend CUDA graph replay.
 * JIT-compiled version for python/sglang/jit_kernel.
 *
 * OVERVIEW:
 * This kernel fuses multiple tensor copy operations (cache_seqlens, cu_seqlens_k,
 * page_table, nsa metadata, and optional FlashMLA metadata) into single kernel
 * launches, significantly reducing kernel launch overhead and improving CUDA
 * graph replay performance during inference.
 *
 * PERFORMANCE BENEFITS:
 * - Single kernel launch vs. multiple separate copies (3-10x faster)
 * - Branch-free execution via template specialization
 * - Optimized memory coalescing and SM utilization
 * - Especially beneficial in CUDA graph replay scenarios
 *
 * DESIGN:
 * - Three specialized kernels for different forward modes (DECODE, TARGET_VERIFY, DRAFT_EXTEND)
 * - Template parameters eliminate runtime branches and enable compile-time optimization
 * - Multi-backend variant copies to 3 destinations in one kernel (for speculative decoding)
 *
 * USAGE:
 * This header is included by JIT compilation system. The FusedMetadataCopyKernel
 * and FusedMetadataCopyMultiKernel wrapper structs provide the Python-accessible interface.
 */

#pragma once

#include <cuda_runtime.h>

// Forward mode enum (must match Python ForwardMode in sglang/srt/layers/attention/nsa_backend.py)
enum ForwardModeEnum { DECODE = 0, TARGET_VERIFY = 1, DRAFT_EXTEND = 2 };

/**
 * Specialized kernel for DECODE mode - optimized for single token decode.
 * Template parameters eliminate runtime branches.
 */
template <bool HAS_REAL_PAGE_TABLE, bool HAS_FLASHMLA>
__global__ void fused_metadata_copy_decode_kernel(
    const int32_t* __restrict__ cache_seqlens_src,
    const int32_t* __restrict__ cu_seqlens_k_src,
    const int32_t* __restrict__ page_indices_src,
    const int32_t* __restrict__ nsa_cache_seqlens_src,
    const int32_t* __restrict__ nsa_cu_seqlens_k_src,
    const int32_t* __restrict__ real_page_table_src,
    const int32_t* __restrict__ flashmla_num_splits_src,
    const int32_t* __restrict__ flashmla_metadata_src,

    int32_t* __restrict__ cache_seqlens_dst,
    int32_t* __restrict__ cu_seqlens_k_dst,
    int32_t* __restrict__ page_table_1_dst,
    int32_t* __restrict__ nsa_cache_seqlens_dst,
    int32_t* __restrict__ nsa_cu_seqlens_k_dst,
    int32_t* __restrict__ real_page_table_dst,
    int32_t* __restrict__ flashmla_num_splits_dst,
    int32_t* __restrict__ flashmla_metadata_dst,

    int bs,
    int max_len,
    int seqlens_expanded_size,
    int page_table_1_stride,
    int real_page_table_cols,
    int real_page_table_dst_stride,
    int flashmla_metadata_size) {
  int tid = blockIdx.x * blockDim.x + threadIdx.x;
  int total_threads = gridDim.x * blockDim.x;

  // Copy cache_seqlens (bs elements)
#pragma unroll 8
  for (int i = tid; i < bs; i += total_threads) {
    cache_seqlens_dst[i] = cache_seqlens_src[i];
  }

  // Copy cu_seqlens_k (skip first element)
#pragma unroll 8
  for (int i = tid; i < bs; i += total_threads) {
    cu_seqlens_k_dst[i + 1] = cu_seqlens_k_src[i + 1];
  }

  // DECODE mode: copy page_table_1 and nsa_cache_seqlens
  int page_table_elements = bs * max_len;
#pragma unroll 4
  for (int i = tid; i < page_table_elements; i += total_threads) {
    int row = i / max_len;
    int col = i % max_len;
    page_table_1_dst[row * page_table_1_stride + col] = page_indices_src[i];
  }

#pragma unroll 8
  for (int i = tid; i < bs; i += total_threads) {
    nsa_cache_seqlens_dst[i] = nsa_cache_seqlens_src[i];
  }

  // Copy NSA cu_seqlens (in decode mode, size == bs)
#pragma unroll 8
  for (int i = tid; i < bs; i += total_threads) {
    nsa_cu_seqlens_k_dst[i + 1] = nsa_cu_seqlens_k_src[i + 1];
  }

  // Copy real page table - compile-time branch
  if (real_page_table_src != nullptr && real_page_table_dst != nullptr) {
    int real_table_elements = bs * real_page_table_cols;
#pragma unroll 2
    for (int i = tid; i < real_table_elements; i += total_threads) {
      int row = i / real_page_table_cols;
      int col = i % real_page_table_cols;
      int src_idx = row * real_page_table_cols + col;
      int dst_idx = row * real_page_table_dst_stride + col;
      real_page_table_dst[dst_idx] = real_page_table_src[src_idx];
    }
  }

  // Copy FlashMLA num_splits and metadata - compile-time branch
  if constexpr (HAS_FLASHMLA) {
    int flashmla_size = bs + 1;
#pragma unroll 8
    for (int i = tid; i < flashmla_size; i += total_threads) {
      flashmla_num_splits_dst[i] = flashmla_num_splits_src[i];
    }

#pragma unroll 2
    for (int i = tid; i < flashmla_metadata_size; i += total_threads) {
      flashmla_metadata_dst[i] = flashmla_metadata_src[i];
    }
  }
}

/**
 * Specialized kernel for TARGET_VERIFY mode - optimized for speculative verification.
 */
template <bool HAS_REAL_PAGE_TABLE, bool HAS_FLASHMLA>
__global__ void fused_metadata_copy_target_verify_kernel(
    const int32_t* __restrict__ cache_seqlens_src,
    const int32_t* __restrict__ cu_seqlens_k_src,
    const int32_t* __restrict__ page_indices_src,
    const int32_t* __restrict__ nsa_cache_seqlens_src,
    const int32_t* __restrict__ seqlens_expanded_src,
    const int32_t* __restrict__ nsa_cu_seqlens_k_src,
    const int32_t* __restrict__ real_page_table_src,
    const int32_t* __restrict__ flashmla_num_splits_src,
    const int32_t* __restrict__ flashmla_metadata_src,

    int32_t* __restrict__ cache_seqlens_dst,
    int32_t* __restrict__ cu_seqlens_k_dst,
    int32_t* __restrict__ page_table_1_dst,
    int32_t* __restrict__ nsa_cache_seqlens_dst,
    int32_t* __restrict__ seqlens_expanded_dst,
    int32_t* __restrict__ nsa_cu_seqlens_k_dst,
    int32_t* __restrict__ real_page_table_dst,
    int32_t* __restrict__ flashmla_num_splits_dst,
    int32_t* __restrict__ flashmla_metadata_dst,

    int bs,
    int max_seqlen_k,
    int seqlens_expanded_size,
    int page_indices_rows,
    int page_table_1_stride,
    int real_page_table_cols,
    int real_page_table_dst_stride,
    int flashmla_metadata_size) {
  int tid = blockIdx.x * blockDim.x + threadIdx.x;
  int total_threads = gridDim.x * blockDim.x;

  // Copy cache_seqlens (bs elements)
#pragma unroll 8
  for (int i = tid; i < bs; i += total_threads) {
    cache_seqlens_dst[i] = cache_seqlens_src[i];
  }

  // Copy cu_seqlens_k (skip first element)
#pragma unroll 8
  for (int i = tid; i < bs; i += total_threads) {
    cu_seqlens_k_dst[i + 1] = cu_seqlens_k_src[i + 1];
  }

  // TARGET_VERIFY mode: copy page_table, seqlens_expanded, and nsa_cache_seqlens
  int page_table_elements = page_indices_rows * max_seqlen_k;
#pragma unroll 4
  for (int i = tid; i < page_table_elements; i += total_threads) {
    int row = i / max_seqlen_k;
    int col = i % max_seqlen_k;
    page_table_1_dst[row * page_table_1_stride + col] = page_indices_src[i];
  }

#pragma unroll 4
  for (int i = tid; i < seqlens_expanded_size; i += total_threads) {
    seqlens_expanded_dst[i] = seqlens_expanded_src[i];
  }

#pragma unroll 4
  for (int i = tid; i < seqlens_expanded_size; i += total_threads) {
    nsa_cache_seqlens_dst[i] = nsa_cache_seqlens_src[i];
  }

  // Copy NSA cu_seqlens
#pragma unroll 4
  for (int i = tid; i < seqlens_expanded_size; i += total_threads) {
    nsa_cu_seqlens_k_dst[i + 1] = nsa_cu_seqlens_k_src[i + 1];
  }

  // Copy real page table - compile-time branch
  if constexpr (HAS_REAL_PAGE_TABLE) {
    int real_table_elements = page_indices_rows * real_page_table_cols;
#pragma unroll 2
    for (int i = tid; i < real_table_elements; i += total_threads) {
      int row = i / real_page_table_cols;
      int col = i % real_page_table_cols;
      real_page_table_dst[row * real_page_table_dst_stride + col] =
          real_page_table_src[row * real_page_table_cols + col];
    }
  }

  // Copy FlashMLA num_splits and metadata - compile-time branch
  if constexpr (HAS_FLASHMLA) {
    int flashmla_size = seqlens_expanded_size + 1;
#pragma unroll 4
    for (int i = tid; i < flashmla_size; i += total_threads) {
      flashmla_num_splits_dst[i] = flashmla_num_splits_src[i];
    }

#pragma unroll 2
    for (int i = tid; i < flashmla_metadata_size; i += total_threads) {
      flashmla_metadata_dst[i] = flashmla_metadata_src[i];
    }
  }
}

/**
 * Specialized kernel for DRAFT_EXTEND mode - optimized for draft token generation.
 */
template <bool HAS_REAL_PAGE_TABLE, bool HAS_FLASHMLA>
__global__ void fused_metadata_copy_draft_extend_kernel(
    const int32_t* __restrict__ cache_seqlens_src,
    const int32_t* __restrict__ cu_seqlens_k_src,
    const int32_t* __restrict__ page_indices_src,
    const int32_t* __restrict__ nsa_cache_seqlens_src,
    const int32_t* __restrict__ seqlens_expanded_src,
    const int32_t* __restrict__ nsa_cu_seqlens_k_src,
    const int32_t* __restrict__ real_page_table_src,
    const int32_t* __restrict__ flashmla_num_splits_src,
    const int32_t* __restrict__ flashmla_metadata_src,

    int32_t* __restrict__ cache_seqlens_dst,
    int32_t* __restrict__ cu_seqlens_k_dst,
    int32_t* __restrict__ page_table_1_dst,
    int32_t* __restrict__ nsa_cache_seqlens_dst,
    int32_t* __restrict__ seqlens_expanded_dst,
    int32_t* __restrict__ nsa_cu_seqlens_k_dst,
    int32_t* __restrict__ real_page_table_dst,
    int32_t* __restrict__ flashmla_num_splits_dst,
    int32_t* __restrict__ flashmla_metadata_dst,

    int bs,
    int max_seqlen_k,
    int seqlens_expanded_size,
    int page_indices_rows,
    int page_table_1_stride,
    int real_page_table_cols,
    int real_page_table_dst_stride,
    int flashmla_metadata_size) {
  int tid = blockIdx.x * blockDim.x + threadIdx.x;
  int total_threads = gridDim.x * blockDim.x;

  // Copy cache_seqlens (bs elements)
#pragma unroll 8
  for (int i = tid; i < bs; i += total_threads) {
    cache_seqlens_dst[i] = cache_seqlens_src[i];
  }

  // Copy cu_seqlens_k (skip first element)
#pragma unroll 8
  for (int i = tid; i < bs; i += total_threads) {
    cu_seqlens_k_dst[i + 1] = cu_seqlens_k_src[i + 1];
  }

  // DRAFT_EXTEND mode: copy page_table, seqlens_expanded, and nsa_cache_seqlens
  int page_table_elements = page_indices_rows * max_seqlen_k;
#pragma unroll 4
  for (int i = tid; i < page_table_elements; i += total_threads) {
    int row = i / max_seqlen_k;
    int col = i % max_seqlen_k;
    page_table_1_dst[row * page_table_1_stride + col] = page_indices_src[i];
  }

#pragma unroll 4
  for (int i = tid; i < seqlens_expanded_size; i += total_threads) {
    seqlens_expanded_dst[i] = seqlens_expanded_src[i];
  }

#pragma unroll 4
  for (int i = tid; i < seqlens_expanded_size; i += total_threads) {
    nsa_cache_seqlens_dst[i] = nsa_cache_seqlens_src[i];
  }

  // Copy NSA cu_seqlens
#pragma unroll 4
  for (int i = tid; i < seqlens_expanded_size; i += total_threads) {
    nsa_cu_seqlens_k_dst[i + 1] = nsa_cu_seqlens_k_src[i + 1];
  }

  // Copy real page table - compile-time branch
  if constexpr (HAS_REAL_PAGE_TABLE) {
    int real_table_elements = page_indices_rows * real_page_table_cols;
#pragma unroll 2
    for (int i = tid; i < real_table_elements; i += total_threads) {
      int row = i / real_page_table_cols;
      int col = i % real_page_table_cols;
      real_page_table_dst[row * real_page_table_dst_stride + col] =
          real_page_table_src[row * real_page_table_cols + col];
    }
  }

  // Copy FlashMLA num_splits and metadata - compile-time branch
  if constexpr (HAS_FLASHMLA) {
    int flashmla_size = seqlens_expanded_size + 1;
#pragma unroll 4
    for (int i = tid; i < flashmla_size; i += total_threads) {
      flashmla_num_splits_dst[i] = flashmla_num_splits_src[i];
    }

#pragma unroll 2
    for (int i = tid; i < flashmla_metadata_size; i += total_threads) {
      flashmla_metadata_dst[i] = flashmla_metadata_src[i];
    }
  }
}

/**
 * Multi-backend specialized kernel for DECODE mode.
 * Copies from one source to THREE destinations in a single kernel launch.
 */
template <bool HAS_REAL_PAGE_TABLE, bool HAS_FLASHMLA>
__global__ void fused_metadata_copy_decode_multi_kernel(
    // Source tensors (shared)
    const int32_t* __restrict__ cache_seqlens_src,
    const int32_t* __restrict__ cu_seqlens_k_src,
    const int32_t* __restrict__ page_indices_src,
    const int32_t* __restrict__ nsa_cache_seqlens_src,
    const int32_t* __restrict__ nsa_cu_seqlens_k_src,
    const int32_t* __restrict__ real_page_table_src,
    const int32_t* __restrict__ flashmla_num_splits_src,
    const int32_t* __restrict__ flashmla_metadata_src,

    // Destination tensors for backend 0
    int32_t* __restrict__ cache_seqlens_dst0,
    int32_t* __restrict__ cu_seqlens_k_dst0,
    int32_t* __restrict__ page_table_1_dst0,
    int32_t* __restrict__ nsa_cache_seqlens_dst0,
    int32_t* __restrict__ nsa_cu_seqlens_k_dst0,
    int32_t* __restrict__ real_page_table_dst0,
    int32_t* __restrict__ flashmla_num_splits_dst0,
    int32_t* __restrict__ flashmla_metadata_dst0,

    // Destination tensors for backend 1
    int32_t* __restrict__ cache_seqlens_dst1,
    int32_t* __restrict__ cu_seqlens_k_dst1,
    int32_t* __restrict__ page_table_1_dst1,
    int32_t* __restrict__ nsa_cache_seqlens_dst1,
    int32_t* __restrict__ nsa_cu_seqlens_k_dst1,
    int32_t* __restrict__ real_page_table_dst1,
    int32_t* __restrict__ flashmla_num_splits_dst1,
    int32_t* __restrict__ flashmla_metadata_dst1,

    // Destination tensors for backend 2
    int32_t* __restrict__ cache_seqlens_dst2,
    int32_t* __restrict__ cu_seqlens_k_dst2,
    int32_t* __restrict__ page_table_1_dst2,
    int32_t* __restrict__ nsa_cache_seqlens_dst2,
    int32_t* __restrict__ nsa_cu_seqlens_k_dst2,
    int32_t* __restrict__ real_page_table_dst2,
    int32_t* __restrict__ flashmla_num_splits_dst2,
    int32_t* __restrict__ flashmla_metadata_dst2,

    // Parameters
    int bs,
    int max_len,
    int seqlens_expanded_size,
    int page_table_1_stride,
    int real_page_table_cols,
    int real_page_table_dst_stride,
    int flashmla_metadata_size) {
  int tid = blockIdx.x * blockDim.x + threadIdx.x;
  int total_threads = gridDim.x * blockDim.x;

  // Copy cache_seqlens to all 3 backends
#pragma unroll 8
  for (int i = tid; i < bs; i += total_threads) {
    int32_t val = cache_seqlens_src[i];
    cache_seqlens_dst0[i] = val;
    cache_seqlens_dst1[i] = val;
    cache_seqlens_dst2[i] = val;
  }

  // Copy cu_seqlens_k to all 3 backends (skip first element)
#pragma unroll 8
  for (int i = tid; i < bs; i += total_threads) {
    int32_t val = cu_seqlens_k_src[i + 1];
    cu_seqlens_k_dst0[i + 1] = val;
    cu_seqlens_k_dst1[i + 1] = val;
    cu_seqlens_k_dst2[i + 1] = val;
  }

  // DECODE mode: copy page_table_1 to all 3 backends
  int page_table_elements = bs * max_len;
#pragma unroll 4
  for (int i = tid; i < page_table_elements; i += total_threads) {
    int row = i / max_len;
    int col = i % max_len;
    int32_t val = page_indices_src[i];
    page_table_1_dst0[row * page_table_1_stride + col] = val;
    page_table_1_dst1[row * page_table_1_stride + col] = val;
    page_table_1_dst2[row * page_table_1_stride + col] = val;
  }

  // Copy nsa_cache_seqlens to all 3 backends
#pragma unroll 8
  for (int i = tid; i < bs; i += total_threads) {
    int32_t val = nsa_cache_seqlens_src[i];
    nsa_cache_seqlens_dst0[i] = val;
    nsa_cache_seqlens_dst1[i] = val;
    nsa_cache_seqlens_dst2[i] = val;
  }

  // Copy NSA cu_seqlens to all 3 backends
#pragma unroll 8
  for (int i = tid; i < bs; i += total_threads) {
    int32_t val = nsa_cu_seqlens_k_src[i + 1];
    nsa_cu_seqlens_k_dst0[i + 1] = val;
    nsa_cu_seqlens_k_dst1[i + 1] = val;
    nsa_cu_seqlens_k_dst2[i + 1] = val;
  }

  // Copy real page table to all 3 backends
  if (real_page_table_src != nullptr && real_page_table_dst0 != nullptr) {
    int real_table_elements = bs * real_page_table_cols;
#pragma unroll 2
    for (int i = tid; i < real_table_elements; i += total_threads) {
      int row = i / real_page_table_cols;
      int col = i % real_page_table_cols;
      int src_idx = row * real_page_table_cols + col;
      int dst_idx = row * real_page_table_dst_stride + col;
      int32_t val = real_page_table_src[src_idx];
      real_page_table_dst0[dst_idx] = val;
      real_page_table_dst1[dst_idx] = val;
      real_page_table_dst2[dst_idx] = val;
    }
  }

  // Copy FlashMLA metadata to all 3 backends
  if constexpr (HAS_FLASHMLA) {
    int flashmla_size = bs + 1;
#pragma unroll 8
    for (int i = tid; i < flashmla_size; i += total_threads) {
      int32_t val = flashmla_num_splits_src[i];
      flashmla_num_splits_dst0[i] = val;
      flashmla_num_splits_dst1[i] = val;
      flashmla_num_splits_dst2[i] = val;
    }

#pragma unroll 2
    for (int i = tid; i < flashmla_metadata_size; i += total_threads) {
      int32_t val = flashmla_metadata_src[i];
      flashmla_metadata_dst0[i] = val;
      flashmla_metadata_dst1[i] = val;
      flashmla_metadata_dst2[i] = val;
    }
  }
}

// ============================================================================
// Host-side launcher wrappers for JIT compilation
// ============================================================================

#include <sgl_kernel/tensor.h>
#include <sgl_kernel/utils.h>

#include <tvm/ffi/container/tensor.h>

#include <algorithm>  // for std::min

namespace {

// Launch configuration constants
constexpr int THREADS_PER_BLOCK = 256;
constexpr int MAX_GRID_SIZE = 1024;  // Limit to prevent excessive resource usage

/**
 * Calculate kernel launch configuration.
 *
 * @param total_work Total number of work items
 * @param threads_per_block Threads per block (default: THREADS_PER_BLOCK)
 * @return Grid dimension for kernel launch
 */
inline dim3 get_launch_config(int total_work, int threads_per_block = THREADS_PER_BLOCK) {
  int num_blocks = (total_work + threads_per_block - 1) / threads_per_block;
  // Limit grid size to prevent excessive resource usage while ensuring coverage
  num_blocks = std::min(num_blocks, MAX_GRID_SIZE);
  return dim3(num_blocks);
}

/**
 * JIT wrapper for single-backend fused metadata copy kernel.
 *
 * This struct provides a unified interface for launching the fused metadata copy
 * kernel with different forward modes. The template parameters allow compile-time
 * specialization to eliminate runtime branches.
 *
 * @tparam FORWARD_MODE Forward mode: 0=DECODE, 1=TARGET_VERIFY, 2=DRAFT_EXTEND
 * @tparam HAS_REAL_PAGE_TABLE Whether real_page_table tensors are present
 * @tparam HAS_FLASHMLA Whether FlashMLA metadata tensors are present
 */
template <int FORWARD_MODE, bool HAS_REAL_PAGE_TABLE, bool HAS_FLASHMLA>
struct FusedMetadataCopyKernel {
  static_assert(
      FORWARD_MODE >= 0 && FORWARD_MODE <= 2,
      "FORWARD_MODE must be 0 (DECODE), 1 (TARGET_VERIFY), or 2 (DRAFT_EXTEND)");

  static void
  run(const tvm::ffi::TensorView cache_seqlens_src,
      const tvm::ffi::TensorView cu_seqlens_k_src,
      const tvm::ffi::TensorView page_indices_src,
      const tvm::ffi::TensorView nsa_cache_seqlens_src,
      const tvm::ffi::TensorView seqlens_expanded_src,
      const tvm::ffi::TensorView nsa_cu_seqlens_k_src,
      const tvm::ffi::TensorView real_page_table_src,
      const tvm::ffi::TensorView flashmla_num_splits_src,
      const tvm::ffi::TensorView flashmla_metadata_src,
      const tvm::ffi::TensorView cache_seqlens_dst,
      const tvm::ffi::TensorView cu_seqlens_k_dst,
      const tvm::ffi::TensorView page_table_1_dst,
      const tvm::ffi::TensorView nsa_cache_seqlens_dst,
      const tvm::ffi::TensorView seqlens_expanded_dst,
      const tvm::ffi::TensorView nsa_cu_seqlens_k_dst,
      const tvm::ffi::TensorView real_page_table_dst,
      const tvm::ffi::TensorView flashmla_num_splits_dst,
      const tvm::ffi::TensorView flashmla_metadata_dst,
      int bs,
      int max_len,
      int max_seqlen_k,
      int seqlens_expanded_size) {
    // Extract raw pointers from TensorView objects
    // Required tensors (always present)
    const int32_t* cache_seqlens_src_ptr = static_cast<const int32_t*>(cache_seqlens_src.data_ptr());
    const int32_t* cu_seqlens_k_src_ptr = static_cast<const int32_t*>(cu_seqlens_k_src.data_ptr());
    const int32_t* page_indices_src_ptr = static_cast<const int32_t*>(page_indices_src.data_ptr());
    const int32_t* nsa_cache_seqlens_src_ptr = static_cast<const int32_t*>(nsa_cache_seqlens_src.data_ptr());
    const int32_t* nsa_cu_seqlens_k_src_ptr = static_cast<const int32_t*>(nsa_cu_seqlens_k_src.data_ptr());

    // Optional source tensors (mode-dependent or feature-dependent)
    const int32_t* seqlens_expanded_src_ptr =
        seqlens_expanded_src.data_ptr() ? static_cast<const int32_t*>(seqlens_expanded_src.data_ptr()) : nullptr;
    const int32_t* real_page_table_src_ptr =
        real_page_table_src.data_ptr() ? static_cast<const int32_t*>(real_page_table_src.data_ptr()) : nullptr;
    const int32_t* flashmla_num_splits_src_ptr =
        flashmla_num_splits_src.data_ptr() ? static_cast<const int32_t*>(flashmla_num_splits_src.data_ptr()) : nullptr;
    const int32_t* flashmla_metadata_src_ptr =
        flashmla_metadata_src.data_ptr() ? static_cast<const int32_t*>(flashmla_metadata_src.data_ptr()) : nullptr;

    // Required destination tensors
    int32_t* cache_seqlens_dst_ptr = static_cast<int32_t*>(cache_seqlens_dst.data_ptr());
    int32_t* cu_seqlens_k_dst_ptr = static_cast<int32_t*>(cu_seqlens_k_dst.data_ptr());
    int32_t* page_table_1_dst_ptr = static_cast<int32_t*>(page_table_1_dst.data_ptr());
    int32_t* nsa_cache_seqlens_dst_ptr = static_cast<int32_t*>(nsa_cache_seqlens_dst.data_ptr());
    int32_t* nsa_cu_seqlens_k_dst_ptr = static_cast<int32_t*>(nsa_cu_seqlens_k_dst.data_ptr());

    // Optional destination tensors
    int32_t* seqlens_expanded_dst_ptr =
        seqlens_expanded_dst.data_ptr() ? static_cast<int32_t*>(seqlens_expanded_dst.data_ptr()) : nullptr;
    int32_t* real_page_table_dst_ptr =
        real_page_table_dst.data_ptr() ? static_cast<int32_t*>(real_page_table_dst.data_ptr()) : nullptr;
    int32_t* flashmla_num_splits_dst_ptr =
        flashmla_num_splits_dst.data_ptr() ? static_cast<int32_t*>(flashmla_num_splits_dst.data_ptr()) : nullptr;
    int32_t* flashmla_metadata_dst_ptr =
        flashmla_metadata_dst.data_ptr() ? static_cast<int32_t*>(flashmla_metadata_dst.data_ptr()) : nullptr;

    // Calculate additional parameters
    int page_indices_rows = page_indices_src.shape()[0];
    int page_table_1_stride = page_table_1_dst.shape()[1];
    int real_page_table_cols = real_page_table_src.data_ptr() ? real_page_table_src.shape()[1] : 0;
    int real_page_table_dst_stride = real_page_table_dst.data_ptr() ? real_page_table_dst.stride(0) : 0;
    int flashmla_metadata_size = flashmla_metadata_src.data_ptr() ? flashmla_metadata_src.numel() : 0;

    // Calculate grid configuration (unified for all modes like verified version)
    int max_elements = std::max(
        {bs,
         page_indices_rows * max_seqlen_k,
         seqlens_expanded_size,
         HAS_FLASHMLA ? (seqlens_expanded_size + 1) : 0,
         HAS_FLASHMLA ? flashmla_metadata_size : 0});

    dim3 grid = get_launch_config(max_elements);
    dim3 block(THREADS_PER_BLOCK);

    if constexpr (FORWARD_MODE == 0) {  // DECODE
      fused_metadata_copy_decode_kernel<HAS_REAL_PAGE_TABLE, HAS_FLASHMLA><<<grid, block>>>(
          cache_seqlens_src_ptr,
          cu_seqlens_k_src_ptr,
          page_indices_src_ptr,
          nsa_cache_seqlens_src_ptr,
          nsa_cu_seqlens_k_src_ptr,
          real_page_table_src_ptr,
          flashmla_num_splits_src_ptr,
          flashmla_metadata_src_ptr,
          cache_seqlens_dst_ptr,
          cu_seqlens_k_dst_ptr,
          page_table_1_dst_ptr,
          nsa_cache_seqlens_dst_ptr,
          nsa_cu_seqlens_k_dst_ptr,
          real_page_table_dst_ptr,
          flashmla_num_splits_dst_ptr,
          flashmla_metadata_dst_ptr,
          bs,
          max_len,
          seqlens_expanded_size,
          page_table_1_stride,
          real_page_table_cols,
          real_page_table_dst_stride,
          flashmla_metadata_size);
    } else if constexpr (FORWARD_MODE == 1) {  // TARGET_VERIFY
      fused_metadata_copy_target_verify_kernel<HAS_REAL_PAGE_TABLE, HAS_FLASHMLA><<<grid, block>>>(
          cache_seqlens_src_ptr,
          cu_seqlens_k_src_ptr,
          page_indices_src_ptr,
          nsa_cache_seqlens_src_ptr,
          seqlens_expanded_src_ptr,
          nsa_cu_seqlens_k_src_ptr,
          real_page_table_src_ptr,
          flashmla_num_splits_src_ptr,
          flashmla_metadata_src_ptr,
          cache_seqlens_dst_ptr,
          cu_seqlens_k_dst_ptr,
          page_table_1_dst_ptr,
          nsa_cache_seqlens_dst_ptr,
          seqlens_expanded_dst_ptr,
          nsa_cu_seqlens_k_dst_ptr,
          real_page_table_dst_ptr,
          flashmla_num_splits_dst_ptr,
          flashmla_metadata_dst_ptr,
          bs,
          max_seqlen_k,
          seqlens_expanded_size,
          page_indices_rows,
          page_table_1_stride,
          real_page_table_cols,
          real_page_table_dst_stride,
          flashmla_metadata_size);
    } else if constexpr (FORWARD_MODE == 2) {  // DRAFT_EXTEND
      fused_metadata_copy_draft_extend_kernel<HAS_REAL_PAGE_TABLE, HAS_FLASHMLA><<<grid, block>>>(
          cache_seqlens_src_ptr,
          cu_seqlens_k_src_ptr,
          page_indices_src_ptr,
          nsa_cache_seqlens_src_ptr,
          seqlens_expanded_src_ptr,
          nsa_cu_seqlens_k_src_ptr,
          real_page_table_src_ptr,
          flashmla_num_splits_src_ptr,
          flashmla_metadata_src_ptr,
          cache_seqlens_dst_ptr,
          cu_seqlens_k_dst_ptr,
          page_table_1_dst_ptr,
          nsa_cache_seqlens_dst_ptr,
          seqlens_expanded_dst_ptr,
          nsa_cu_seqlens_k_dst_ptr,
          real_page_table_dst_ptr,
          flashmla_num_splits_dst_ptr,
          flashmla_metadata_dst_ptr,
          bs,
          max_seqlen_k,
          seqlens_expanded_size,
          page_indices_rows,
          page_table_1_stride,
          real_page_table_cols,
          real_page_table_dst_stride,
          flashmla_metadata_size);
    }
  }
};

/**
 * JIT wrapper for multi-backend fused metadata copy kernel.
 *
 * This kernel optimizes the common case where metadata needs to be copied from
 * one source to THREE destination backends in a single kernel launch. This is
 * 3x faster than launching three separate kernels due to:
 * - Reduced kernel launch overhead
 * - Improved memory coalescing (source read once, written to 3 destinations)
 * - Better GPU occupancy
 *
 * Currently only supports DECODE mode, which is the most frequently used mode
 * in speculative decoding with multiple backends.
 *
 * @tparam HAS_REAL_PAGE_TABLE Whether real_page_table tensors are present
 * @tparam HAS_FLASHMLA Whether FlashMLA metadata tensors are present
 */
template <bool HAS_REAL_PAGE_TABLE, bool HAS_FLASHMLA>
struct FusedMetadataCopyMultiKernel {
  static void
  run(const tvm::ffi::TensorView cache_seqlens_src,
      const tvm::ffi::TensorView cu_seqlens_k_src,
      const tvm::ffi::TensorView page_indices_src,
      const tvm::ffi::TensorView nsa_cache_seqlens_src,
      const tvm::ffi::TensorView nsa_cu_seqlens_k_src,
      const tvm::ffi::TensorView real_page_table_src,
      const tvm::ffi::TensorView flashmla_num_splits_src,
      const tvm::ffi::TensorView flashmla_metadata_src,
      const tvm::ffi::TensorView cache_seqlens_dst0,
      const tvm::ffi::TensorView cu_seqlens_k_dst0,
      const tvm::ffi::TensorView page_table_1_dst0,
      const tvm::ffi::TensorView nsa_cache_seqlens_dst0,
      const tvm::ffi::TensorView nsa_cu_seqlens_k_dst0,
      const tvm::ffi::TensorView real_page_table_dst0,
      const tvm::ffi::TensorView flashmla_num_splits_dst0,
      const tvm::ffi::TensorView flashmla_metadata_dst0,
      const tvm::ffi::TensorView cache_seqlens_dst1,
      const tvm::ffi::TensorView cu_seqlens_k_dst1,
      const tvm::ffi::TensorView page_table_1_dst1,
      const tvm::ffi::TensorView nsa_cache_seqlens_dst1,
      const tvm::ffi::TensorView nsa_cu_seqlens_k_dst1,
      const tvm::ffi::TensorView real_page_table_dst1,
      const tvm::ffi::TensorView flashmla_num_splits_dst1,
      const tvm::ffi::TensorView flashmla_metadata_dst1,
      const tvm::ffi::TensorView cache_seqlens_dst2,
      const tvm::ffi::TensorView cu_seqlens_k_dst2,
      const tvm::ffi::TensorView page_table_1_dst2,
      const tvm::ffi::TensorView nsa_cache_seqlens_dst2,
      const tvm::ffi::TensorView nsa_cu_seqlens_k_dst2,
      const tvm::ffi::TensorView real_page_table_dst2,
      const tvm::ffi::TensorView flashmla_num_splits_dst2,
      const tvm::ffi::TensorView flashmla_metadata_dst2,
      int bs,
      int max_len,
      int seqlens_expanded_size) {
    // Extract source pointers (shared across all 3 destination backends)
    const int32_t* cache_seqlens_src_ptr = static_cast<const int32_t*>(cache_seqlens_src.data_ptr());
    const int32_t* cu_seqlens_k_src_ptr = static_cast<const int32_t*>(cu_seqlens_k_src.data_ptr());
    const int32_t* page_indices_src_ptr = static_cast<const int32_t*>(page_indices_src.data_ptr());
    const int32_t* nsa_cache_seqlens_src_ptr = static_cast<const int32_t*>(nsa_cache_seqlens_src.data_ptr());
    const int32_t* nsa_cu_seqlens_k_src_ptr = static_cast<const int32_t*>(nsa_cu_seqlens_k_src.data_ptr());

    // Optional source tensors
    const int32_t* real_page_table_src_ptr =
        real_page_table_src.data_ptr() ? static_cast<const int32_t*>(real_page_table_src.data_ptr()) : nullptr;
    const int32_t* flashmla_num_splits_src_ptr =
        flashmla_num_splits_src.data_ptr() ? static_cast<const int32_t*>(flashmla_num_splits_src.data_ptr()) : nullptr;
    const int32_t* flashmla_metadata_src_ptr =
        flashmla_metadata_src.data_ptr() ? static_cast<const int32_t*>(flashmla_metadata_src.data_ptr()) : nullptr;

    // Extract destination pointers for backend 0 (first speculative step)
    int32_t* cache_seqlens_dst0_ptr = static_cast<int32_t*>(cache_seqlens_dst0.data_ptr());
    int32_t* cu_seqlens_k_dst0_ptr = static_cast<int32_t*>(cu_seqlens_k_dst0.data_ptr());
    int32_t* page_table_1_dst0_ptr = static_cast<int32_t*>(page_table_1_dst0.data_ptr());
    int32_t* nsa_cache_seqlens_dst0_ptr = static_cast<int32_t*>(nsa_cache_seqlens_dst0.data_ptr());
    int32_t* nsa_cu_seqlens_k_dst0_ptr = static_cast<int32_t*>(nsa_cu_seqlens_k_dst0.data_ptr());
    int32_t* real_page_table_dst0_ptr =
        real_page_table_dst0.data_ptr() ? static_cast<int32_t*>(real_page_table_dst0.data_ptr()) : nullptr;
    int32_t* flashmla_num_splits_dst0_ptr =
        flashmla_num_splits_dst0.data_ptr() ? static_cast<int32_t*>(flashmla_num_splits_dst0.data_ptr()) : nullptr;
    int32_t* flashmla_metadata_dst0_ptr =
        flashmla_metadata_dst0.data_ptr() ? static_cast<int32_t*>(flashmla_metadata_dst0.data_ptr()) : nullptr;

    // Extract destination pointers for backend 1 (second speculative step)
    int32_t* cache_seqlens_dst1_ptr = static_cast<int32_t*>(cache_seqlens_dst1.data_ptr());
    int32_t* cu_seqlens_k_dst1_ptr = static_cast<int32_t*>(cu_seqlens_k_dst1.data_ptr());
    int32_t* page_table_1_dst1_ptr = static_cast<int32_t*>(page_table_1_dst1.data_ptr());
    int32_t* nsa_cache_seqlens_dst1_ptr = static_cast<int32_t*>(nsa_cache_seqlens_dst1.data_ptr());
    int32_t* nsa_cu_seqlens_k_dst1_ptr = static_cast<int32_t*>(nsa_cu_seqlens_k_dst1.data_ptr());
    int32_t* real_page_table_dst1_ptr =
        real_page_table_dst1.data_ptr() ? static_cast<int32_t*>(real_page_table_dst1.data_ptr()) : nullptr;
    int32_t* flashmla_num_splits_dst1_ptr =
        flashmla_num_splits_dst1.data_ptr() ? static_cast<int32_t*>(flashmla_num_splits_dst1.data_ptr()) : nullptr;
    int32_t* flashmla_metadata_dst1_ptr =
        flashmla_metadata_dst1.data_ptr() ? static_cast<int32_t*>(flashmla_metadata_dst1.data_ptr()) : nullptr;

    // Extract destination pointers for backend 2 (third speculative step)
    int32_t* cache_seqlens_dst2_ptr = static_cast<int32_t*>(cache_seqlens_dst2.data_ptr());
    int32_t* cu_seqlens_k_dst2_ptr = static_cast<int32_t*>(cu_seqlens_k_dst2.data_ptr());
    int32_t* page_table_1_dst2_ptr = static_cast<int32_t*>(page_table_1_dst2.data_ptr());
    int32_t* nsa_cache_seqlens_dst2_ptr = static_cast<int32_t*>(nsa_cache_seqlens_dst2.data_ptr());
    int32_t* nsa_cu_seqlens_k_dst2_ptr = static_cast<int32_t*>(nsa_cu_seqlens_k_dst2.data_ptr());
    int32_t* real_page_table_dst2_ptr =
        real_page_table_dst2.data_ptr() ? static_cast<int32_t*>(real_page_table_dst2.data_ptr()) : nullptr;
    int32_t* flashmla_num_splits_dst2_ptr =
        flashmla_num_splits_dst2.data_ptr() ? static_cast<int32_t*>(flashmla_num_splits_dst2.data_ptr()) : nullptr;
    int32_t* flashmla_metadata_dst2_ptr =
        flashmla_metadata_dst2.data_ptr() ? static_cast<int32_t*>(flashmla_metadata_dst2.data_ptr()) : nullptr;

    // Calculate additional parameters
    int page_table_1_stride = page_table_1_dst0.shape()[1];
    int real_page_table_cols = real_page_table_src.data_ptr() ? real_page_table_src.shape()[1] : 0;
    int real_page_table_dst_stride = real_page_table_dst0.data_ptr() ? real_page_table_dst0.stride(0) : 0;
    int flashmla_metadata_size = flashmla_metadata_src.data_ptr() ? flashmla_metadata_src.numel() : 0;

    dim3 grid = get_launch_config(bs * max_len);
    dim3 block(THREADS_PER_BLOCK);

    fused_metadata_copy_decode_multi_kernel<HAS_REAL_PAGE_TABLE, HAS_FLASHMLA><<<grid, block>>>(
        cache_seqlens_src_ptr,
        cu_seqlens_k_src_ptr,
        page_indices_src_ptr,
        nsa_cache_seqlens_src_ptr,
        nsa_cu_seqlens_k_src_ptr,
        real_page_table_src_ptr,
        flashmla_num_splits_src_ptr,
        flashmla_metadata_src_ptr,
        cache_seqlens_dst0_ptr,
        cu_seqlens_k_dst0_ptr,
        page_table_1_dst0_ptr,
        nsa_cache_seqlens_dst0_ptr,
        nsa_cu_seqlens_k_dst0_ptr,
        real_page_table_dst0_ptr,
        flashmla_num_splits_dst0_ptr,
        flashmla_metadata_dst0_ptr,
        cache_seqlens_dst1_ptr,
        cu_seqlens_k_dst1_ptr,
        page_table_1_dst1_ptr,
        nsa_cache_seqlens_dst1_ptr,
        nsa_cu_seqlens_k_dst1_ptr,
        real_page_table_dst1_ptr,
        flashmla_num_splits_dst1_ptr,
        flashmla_metadata_dst1_ptr,
        cache_seqlens_dst2_ptr,
        cu_seqlens_k_dst2_ptr,
        page_table_1_dst2_ptr,
        nsa_cache_seqlens_dst2_ptr,
        nsa_cu_seqlens_k_dst2_ptr,
        real_page_table_dst2_ptr,
        flashmla_num_splits_dst2_ptr,
        flashmla_metadata_dst2_ptr,
        bs,
        max_len,
        seqlens_expanded_size,
        page_table_1_stride,
        real_page_table_cols,
        real_page_table_dst_stride,
        flashmla_metadata_size);
  }
};

}  // namespace
